{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad1db117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup - minimal imports and notes\n",
    "# Purpose: keep imports minimal for exploration and analysis\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Note: `transformers`, `torch`, and plotting libs are optional for later steps\n",
    "# TODO: install required packages if needed: `pip install transformers torch matplotlib`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6498d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Download dataset (default kagglehub cache)\n",
    "# TODO: ensure you have kaggle credentials in ~/.kaggle/kaggle.json\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "except Exception as e:\n",
    "    print('kagglehub not installed; please install: pip install kagglehub')\n",
    "\n",
    "# Download dataset (will use default cache: ~/.cache/kagglehub)\n",
    "# NOTE: this cell intentionally performs download to default cache\n",
    "# to keep reproducible behavior across environments\n",
    "\n",
    "def download_dataset(dataset_id: str = \"nghigia/hcmc-aqi-traffic-camera-dataset\") -> str:\n",
    "    \"\"\"Download dataset via kagglehub.dataset_download and return path.\n",
    "\n",
    "    Returns:\n",
    "        path (str): local path to downloaded dataset (version folder)\n",
    "    \"\"\"\n",
    "    # TODO: implement actual download call\n",
    "    raise NotImplementedError(\"TODO: call kagglehub.dataset_download and return path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "548f88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Verify files & basic metadata\n",
    "# Purpose: list top-level folders, counts, and sample filenames\n",
    "\n",
    "def summarize_data_folder(path: str, max_folders: int = 20) -> dict:\n",
    "    \"\"\"Return a summary dict with folder->num_images and sample files.\n",
    "\n",
    "    TODOs:\n",
    "    - list folders in path\n",
    "    - for each folder count images and gather a few sample names\n",
    "    - return a dict\n",
    "    \"\"\"\n",
    "    # TODO: implement summarization\n",
    "    raise NotImplementedError(\"TODO: implement summarize_data_folder\")\n",
    "\n",
    "\n",
    "# Example usage (learner):\n",
    "# dataset_path = download_dataset()\n",
    "# summary = summarize_data_folder(dataset_path)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5e085",
   "metadata": {},
   "source": [
    "# Answer only these questions (donâ€™t model yet):\n",
    "\n",
    "## 1ï¸âƒ£ What does each folder represent?\n",
    "\n",
    "Is it a single camera?\n",
    "\n",
    "A location?\n",
    "\n",
    "A time span?\n",
    "\n",
    "Check:\n",
    "\n",
    "Number of images per folder\n",
    "\n",
    "Timestamps (from filenames or JSON)\n",
    "\n",
    "Resolution consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca15b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Analysis skeleton - answer the core questions\n",
    "# Purpose: Provide function stubs to determine camera/ location/ timespan\n",
    "\n",
    "\n",
    "def count_images_per_folder(folder_path: str) -> dict:\n",
    "    \"\"\"Return mapping folder->image count.\n",
    "\n",
    "    Expected: {folder_name: count}\n",
    "    TODO: implement using os.listdir\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement count_images_per_folder\")\n",
    "\n",
    "\n",
    "def extract_timestamps_from_filenames(folder_path: str, pattern: str = None) -> list:\n",
    "    \"\"\"Attempt to parse timestamps from filenames.\n",
    "\n",
    "    Hints:\n",
    "    - filenames may contain UNIX timestamps or formatted strings\n",
    "    - consider regex search for 4+ digit year or long epoch numbers\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement extract_timestamps_from_filenames\")\n",
    "\n",
    "\n",
    "def check_resolution_consistency(folder_path: str, sample_n: int = 5) -> dict:\n",
    "    \"\"\"Sample images and return resolutions seen per folder.\n",
    "\n",
    "    Returns: {folder_name: set((w,h), ...)}\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement check_resolution_consistency\")\n",
    "\n",
    "# Example flow (learner):\n",
    "# counts = count_images_per_folder(dataset_path)\n",
    "# timestamps = extract_timestamps_from_filenames(some_folder)\n",
    "# resolutions = check_resolution_consistency(some_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b874e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Utility - identify corrupted small PNGs (3KB or smaller)\n",
    "# Purpose: Provide reusable function; removal left to user\n",
    "\n",
    "def find_small_pngs(root_path: str, size_threshold_bytes: int = 3072) -> list:\n",
    "    \"\"\"Return list of filepaths for PNGs <= threshold.\n",
    "\n",
    "    Learner action: call this, inspect results, and optionally remove files.\n",
    "    \"\"\"\n",
    "    small_files = []\n",
    "    for folder_name in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if fname.lower().endswith('.png'):\n",
    "                fp = os.path.join(folder_path, fname)\n",
    "                try:\n",
    "                    if os.path.getsize(fp) <= size_threshold_bytes:\n",
    "                        small_files.append(fp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return small_files\n",
    "\n",
    "# Example usage:\n",
    "# smalls = find_small_pngs(dataset_path)\n",
    "# print(len(smalls))\n",
    "# To remove (careful):\n",
    "# for f in smalls:\n",
    "#     os.remove(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c726dc2",
   "metadata": {},
   "source": [
    "PNG that are unusable for modeling (e.g., corrupted files) are 3KB or smaller. These should be removed from the dataset before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8126124",
   "metadata": {},
   "source": [
    "\n",
    "## 2ï¸âƒ£ Are there obvious failure frames?\n",
    "\n",
    "Manually inspect:\n",
    "\n",
    "Black frames\n",
    "\n",
    "Extreme blur\n",
    "\n",
    "Obstruction\n",
    "\n",
    "Overexposure\n",
    "\n",
    "Frozen scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Frozen frame detection (exact duplicates)\n",
    "# Purpose: detect identical frames by hashing files\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "def hash_file(path: str) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def find_exact_duplicates(folder_path: str) -> dict:\n",
    "    \"\"\"Return dict of hash -> [filepaths] for duplicates.\n",
    "\n",
    "    Only identical (byte-wise) duplicates are detected. For near-duplicates\n",
    "    or static scenes, consider perceptual hashing or frame-diff methods.\n",
    "    \"\"\"\n",
    "    hashes = defaultdict(list)\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for fname in files:\n",
    "            if not fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "            fp = os.path.join(root, fname)\n",
    "            try:\n",
    "                h = hash_file(fp)\n",
    "                hashes[h].append(fp)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {h: ps for h, ps in hashes.items() if len(ps) > 1}\n",
    "\n",
    "# Example usage:\n",
    "# dups = find_exact_duplicates(dataset_path)\n",
    "# for h, paths in dups.items(): print(h, len(paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13a861a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted Images Found: 0\n",
      "Total Size of Corrupted Files: 0.00 KB\n",
      "\n",
      "No corrupted files found!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Identify and Remove Corrupted Images\n",
    "# PURPOSE: Find and remove PNG files that are 3KB or smaller (corrupted/unusable)\n",
    "# THRESHOLD: 3KB (3072 bytes) - files below this are considered corrupted\n",
    "\n",
    "# Ensure `data_path` is defined. Try common kagglehub cache locations.\n",
    "try:\n",
    "    _ = data_path  # noqa: F821\n",
    "except NameError:\n",
    "    dataset_name = 'hcmc-aqi-traffic-camera-dataset'\n",
    "    possible_roots = [\n",
    "        os.path.expanduser('~/.cache/kagglehub'),\n",
    "        '/Users/ianmorris/Documents/Projects/data/kagglehub'\n",
    "    ]\n",
    "    found = None\n",
    "    for root in possible_roots:\n",
    "        if os.path.exists(root):\n",
    "            for walk_root, dirs, files in os.walk(root):\n",
    "                # look for a 'datasets' subfolder containing our dataset\n",
    "                if 'datasets' in dirs and dataset_name in walk_root:\n",
    "                    candidate = os.path.join(walk_root, 'datasets')\n",
    "                    if os.path.exists(candidate):\n",
    "                        found = candidate\n",
    "                        break\n",
    "            if found:\n",
    "                break\n",
    "    if found:\n",
    "        data_path = found\n",
    "        print(f\"Auto-detected data_path: {data_path}\")\n",
    "    else:\n",
    "        raise NameError(\"data_path not defined. Set `data_path` to your dataset folder or run the download cell.\")\n",
    "\n",
    "# Find all PNG files and check file sizes\n",
    "corrupted_files = []\n",
    "total_size_removed = 0\n",
    "corrupted_count = 0\n",
    "\n",
    "# Walk through all image folders\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        # Check each PNG file in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.png'):\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                file_size = os.path.getsize(filepath)\n",
    "                \n",
    "                # Check if file is corrupted (3KB or smaller)\n",
    "                if file_size <= 3072:  # 3KB in bytes\n",
    "                    corrupted_files.append({\n",
    "                        'path': filepath,\n",
    "                        'folder': folder_name,\n",
    "                        'filename': filename,\n",
    "                        'size': file_size\n",
    "                    })\n",
    "                    corrupted_count += 1\n",
    "                    total_size_removed += file_size\n",
    "\n",
    "# Display results\n",
    "print(f\"Corrupted Images Found: {corrupted_count}\")\n",
    "print(f\"Total Size of Corrupted Files: {total_size_removed / 1024:.2f} KB\\n\")\n",
    "\n",
    "if corrupted_files:\n",
    "    print(\"Corrupted files by folder:\")\n",
    "    folder_corruption = {}\n",
    "    for file_info in corrupted_files:\n",
    "        folder = file_info['folder']\n",
    "        if folder not in folder_corruption:\n",
    "            folder_corruption[folder] = 0\n",
    "        folder_corruption[folder] += 1\n",
    "    \n",
    "    for folder, count in sorted(folder_corruption.items()):\n",
    "        print(f\"  {folder}: {count} corrupted files\")\n",
    "    \n",
    "    # Ask if you want to remove them (optional)\n",
    "    print(\"\\nTo remove corrupted files, uncomment and run the code below:\")\n",
    "    print(\"# for file_info in corrupted_files:\")\n",
    "    print(\"#     os.remove(file_info['path'])\")\n",
    "    print(\"#     print(f\\\\\\\"Removed: {file_info['folder']}/{file_info['filename']}\\\\\\\")\")\n",
    "else:\n",
    "    print(\"No corrupted files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa63b4",
   "metadata": {},
   "source": [
    "## Search for Frozen Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14e160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization skeleton\n",
    "# Purpose: helper to display a grid of images given file paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_image_grid(paths: list, cols: int = 4, thumb_size: tuple[int, int] = (224, 224)):\n",
    "    \"\"\"Display images in a grid.\n",
    "\n",
    "    TODO: implement resizing and plotting; this is a convenience for\n",
    "    quickly verifying retrieved/top-k examples.\n",
    "    \"\"\"\n",
    "    # TODO: implement plotting\n",
    "    raise NotImplementedError(\"TODO: implement show_image_grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31c3c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicate files (frozen frames) found:\n",
      "\n",
      "ðŸ“ NVL_HTP2:\n",
      "  Hash: 637260e06c707ddb152e19b1df3e0e2f (2 copies)\n",
      "    - 20-10-2025-11-03-44.png\n",
      "    - 20-10-2025-10-58-34.png\n",
      "  Hash: 767d913ef1fddcbecbc013891a2db774 (3 copies)\n",
      "    - 20-10-2025-12-00-33.png\n",
      "    - 20-10-2025-11-55-23.png\n",
      "    - 20-10-2025-11-50-15.png\n",
      "  Hash: 5fee10b69c14bde6742bd5d10b61cefd (3 copies)\n",
      "    - 19-10-2025-12-48-02.png\n",
      "    - 19-10-2025-12-42-50.png\n",
      "    - 19-10-2025-12-53-12.png\n",
      "  Hash: 34ec44943f254cdf27b846f3a83bf6bf (2 copies)\n",
      "    - 19-10-2025-12-11-36.png\n",
      "    - 19-10-2025-12-16-48.png\n",
      "\n",
      "ðŸ“ LXO_CTL:\n",
      "  Hash: 16afa6ace8e665561d11ef9d884aa1a7 (3 copies)\n",
      "    - 20-10-2025-11-45-04.png\n",
      "    - 20-10-2025-11-39-56.png\n",
      "    - 20-10-2025-11-50-14.png\n",
      "  Hash: 1217c1d2ccdf262a10f9bc9d5ba7569b (3 copies)\n",
      "    - 19-10-2025-13-24-20.png\n",
      "    - 19-10-2025-13-19-06.png\n",
      "    - 19-10-2025-13-13-53.png\n",
      "  Hash: 8e98be4198169cf1d16a6c09a12802b0 (2 copies)\n",
      "    - 19-10-2025-00-46-14.png\n",
      "    - 19-10-2025-00-41-05.png\n",
      "  Hash: 89439078eff719560048f77d03a4080d (2 copies)\n",
      "    - 20-10-2025-12-21-15.png\n",
      "    - 20-10-2025-12-16-03.png\n",
      "  Hash: eaaf7bb87deef49e29deb25f82ac24e7 (2 copies)\n",
      "    - 20-10-2025-09-45-49.png\n",
      "    - 20-10-2025-09-40-38.png\n",
      "  Hash: 707285d1086887dd3ce5ba8acaa94762 (3 copies)\n",
      "    - 20-10-2025-08-43-50.png\n",
      "    - 20-10-2025-08-48-59.png\n",
      "    - 20-10-2025-08-38-40.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scan each folder for exact duplicates (frozen frames)\n",
    "all_duplicates = {}\n",
    "\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        duplicate_files = find_exact_duplicates(folder_path)\n",
    "        if duplicate_files:\n",
    "            all_duplicates[folder_name] = duplicate_files\n",
    "\n",
    "# Print the results\n",
    "if all_duplicates:\n",
    "    print(\"Exact duplicate files (frozen frames) found:\\n\")\n",
    "    for folder_name, duplicate_files in all_duplicates.items():\n",
    "        print(f\"ðŸ“ {folder_name}:\")\n",
    "        for file_hash, file_paths in duplicate_files.items():\n",
    "            print(f\"  Hash: {file_hash} ({len(file_paths)} copies)\")\n",
    "            for path in file_paths:\n",
    "                print(f\"    - {os.path.basename(path)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âœ“ No exact duplicate files (frozen frames) found in any folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfb0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
